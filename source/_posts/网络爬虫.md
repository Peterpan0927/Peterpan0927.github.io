---
title: 网络爬虫
date: 2017-05-12 19:48:24
tags: 基础介绍 正则匹配
author: Peterpan

---
学习笔记
<!--more-->
# 网络爬虫

什么是爬虫？

网络爬虫（又被称为网页蜘蛛，网络机器人，在FOAF社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动的抓取万维网信息的程序或者脚本。
简单的来说，爬虫就是把别人网站的信息弄到自己的电脑上，再做一些过滤，筛选，归纳，整理，排序等等，如果数据量足够大，算法足够好，能给别人提供优质的检索服务，就可以做成类似google或baidu

为什么选择python写爬虫？

1）抓取网页本身的接口
相比与其他静态编程语言，如java，c#，C++，python抓取网页文档的接口更简洁；相比其他动态脚本语言，如perl，shell，python的urllib2包提供了较为完整的访问网页文档的API。（当然ruby也是很好的选择）
此外，抓取网页有时候需要模拟浏览器的行为，很多网站对于生硬的爬虫抓取都是封杀的。这是我们需要模拟user agent的行为构造合适的请求，譬如模拟用户登陆、模拟session/cookie的存储和设置。在python里都有非常优秀的第三方包帮你搞定，如Requests，mechanize等等。

2）网页抓取后的处理
抓取的网页通常需要处理，比如过滤html标签，提取文本等。python的beautifulsoap提供了简洁的文档处理功能，能用极短的代码完成大部分文档的处理。

其实以上功能很多语言和工具都能做，但是用python能够干得最快，最干净。

## 1.urlopen:
打开一个url方法，返回一个文件对象，然后就可以进行类似文件对象的操作。模块：urllib

![](http://omg5mjb8v.bkt.clouddn.com/AE9908A0-44A5-4B78-8557-360650F1CFFF.png)

## 2.urlretrieve():
urlretrieve方法将url定位到的html文件下载到你的本地硬盘当中,模块：utllib,当没有指定路径的时候可以放到临时路径下面

```python
import urllib
a = urllib.urlretrieve("xxx",filename = "/home/xx/xx/xx.xx")
#将a保存在本地硬盘中，可用的方法和urlopen相同,可以选择保存的路径
```

## 3.使用正则获取图片并保存在本地

```python
import re
imgList = re.findall(r'src="(.*?\.(jpg|png))"',html)
x = 0
for imgurl in imgList:
    print('正在下载%s'%imgurl[0])
    urllib.urlretrieve(imgurl[0],'./downloads/%d.jpg'%x)
    x += 1
```

## 4.urlencode,GET和POST方法

最重要的区别是GET方式是直接以链接形式访问，链接中包含了所有的参数，当然如果包含了密码的话是一种不安全的选择，不过你可以直观地看到自己提交了什么内容。POST则不会在网址上显示所有的参数，不过如果你想直接查看提交了什么就不太方便了。

```python
import urllib
import urllib2
 
values = {}
values['username'] = "1016903103@qq.com"
values['password'] = "XXXX"
data = urllib.urlencode(values) 
url = "http://passport.csdn.net/account/login?from=http://my.csdn.net/my/mycsdn"
request = urllib2.Request(url,data)
response = urllib2.urlopen(request)
print response.read()
#POST方法，构建request时传入两个参数，url和data
```

```python
import urllib2
import urllib
values={}
values['username'] = "1016903103@qq.com"
values['password']="XXXX"
data = urllib.urlencode(values) 
url = "http://passport.csdn.net/account/login"
geturl = url + "?"+data
request = urllib2.Request(geturl)
response = urllib2.urlopen(request)
print response.read()
#GET方法，直接把参数写到网址上面，直接构建一个带参数的URL出来即可。
```





## 5.urllib2和伪造请求头部
目的：是服务器分不清你是爬虫还是浏览器

![](http://omg5mjb8v.bkt.clouddn.com/37300744-19C5-4DD5-8D90-606A58200F0A.png)

设置Headers:

```python
import urllib
import urllib2
url = 'http://www.server.com/login'
user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4)'
values = {'username' : 'pzp', 'password' : 'ascndksv'}
headers = {'User-Agent' : user_agent}
data = urllib.urlencode(value)
request = urllib.Request(url, data, headers)
response = urllib.urlopen(request)
page = response.read()
#服务器会识别headers中的referer是不是它自己，如果不是，有的服务器不会响应，所以我们还可以在headers中加入referer,这样就可以应付防盗链了
headers = {'User-Agent' : user_agent, 'Referer' : 'xxxxxx'}
```

关于headers的其他属性：

> User-Agent : 有些服务器或 Proxy 会通过该值来判断是否是浏览器发出的请求

> Content-Type : 在使用 REST 接口时，服务器会检查该值，用来确定 HTTP Body 中的内容该怎样解析。

> application/xml ： 在 XML RPC，如 RESTful/SOAP 调用时使用

> application/json ： 在 JSON RPC 调用时使用

> application/x-www-form-urlencoded ： 浏览器提交 Web 表单时使用

> 在使用服务器提供的 RESTful 或 SOAP 服务时， Content-Type 设置错误会导致服务器拒绝服务



## 6.BeautifulSoup[参考文章](http://www.cnblogs.com/yupeng/p/3362031.html)

Beautiful Soup 是用Python写的一个HTML/XML的解析器，它可以很好的处理不规范标记并生成剖析树(parse tree)。 它提供简单又常用的导航（navigating），搜索以及修改剖析树的操作。它可以大大节省你的编程时间。
```python
from bs4 import BeautifulSoup
import urllib2
html = urllib2.urlopen("xxxxx")
html = html.read()
soup = BeautifulSoup(html)
#下面是几个常用的功能
soup.标签名  #只会显示第一个
soup.select('标签名')
soup.select('.类名')
soup.select('#id名')
#可以通过标签名，类名，id名来寻找，可以找出所有的
soup.select('标签名 id')  #组合查找
soup.select('head > title') #子标签查找
soup.select('a[class="sister"]')  #属性查找
soup.标签名.string   #获取文字
soup.标签名.attrs    #获取属性
#通过遍历树来寻找
soup.find_all('name, attrs, recursive, text, limit, **kwargs')
#get_text
soup.get_text()#获取文字信息，类似于string，但是string只能对一个对象使用
soup.stripped_string#类似于get_text（）方法，但是会获取所有子标签的文字信息
```



## 7.使用select不断筛选,取得属性

```python
#之前的过程省略,此时yy是一个列表，不能对他进行筛选操作
yy = soup select('div[id=xxx]')
#此时将列表又转换成了对象，可以继续操作了
zz = yy[0]
#指向性的提取对象中的属性
zz['href']
#将对象转换成列表
list(xxx)
```



## 8.urlopen的分析

```python
urlopen(url, data, timeout)
#第一个参数url即为URL，第二个参数data是访问URL时要传送的数据，第三个timeout是设置超时时间。

#第二三个参数是可以不传送的，data默认为空None，timeout默认为 socket._GLOBAL_DEFAULT_TIMEOUT

#第一个参数URL是必须要传送的
```



## 9.构造Request

其实上面的urlopen参数可以传入一个request请求,它其实就是一个Request类的实例，构造时需要传入Url,Data等等的内容。因为在构建请求时还需要加入好多内容，通过构建一个request，服务器响应请求得到应答，这样显得逻辑上清晰明确。

```python
import urllib2
 
request = urllib2.Request("http://www.baidu.com")
response = urllib2.urlopen(request)
print response.read()
```



## 10.Proxy（代理）的设置

urllib2 默认会使用环境变量 http_proxy 来设置 HTTP Proxy。假如一个网站它会检测某一段时间某个IP 的访问次数，如果访问次数过多，它会禁止你的访问。所以你可以设置一些代理服务器来帮助你做工作，每隔一段时间换一个代理，就不知道到底是谁了。

```python
import urllib2
enable_proxy = True
proxy_handler = urllib2.ProxyHandler({"http" : 'http://some-proxy.com:8080'})
null_proxy_handler = urllib2.ProxyHandler({})
if enable_proxy:
    opener = urllib2.build_opener(proxy_handler)
else:
    opener = urllib2.build_opener(null_proxy_handler)
urllib2.install_opener(opener)
```

## 11.Timeout设置

可以设置等待多久超时，为了解决一些网站实在响应过慢而造成的影响。如果第二个参数data为空那么要特别指定是timeout是多少，写明形参，如果data已经传入，则不必声明。

```python
import urllib2
response = urllib2.urlopen('http://www.baidu.com', data, 10)
import urllib2
response = urllib2.urlopen('http://www.baidu.com', timeout=10)
```

## 12.PUT和DELETE方法

PUT：这个方法比较少见。HTML表单也不支持这个。本质上来讲， PUT和POST极为相似，都是向服务器发送数据，但它们之间有一个重要区别，PUT通常指定了资源的存放位置，而POST则没有，POST的数据存放位置由服务器自己决定DELETE：删除某一个资源。基本上这个也很少见，不过还是有一些地方比如amazon的S3云服务里面就用的这个方法来删除资源。

## 13.URLError

产生原因：

- 网络无连接，即本机无法上网
- 连接不到特定的服务器
- 服务器不存在

在代码中可以通过捕获异常来判断原因：

```python
import urllib2

requset = urllib2.Request('http://www.xxxxx.com')
try:
    urllib2.urlopen(request)
except urllib2.URLError, e:
    print e.reason
#如果访问了一个不存在的网址，那么运行的结果是：[Errno 11004] getaddrinfo failed
```

HTTPError,是URLError的子类，所以也可以将父类捕获异常写在子类的后面

```python
import urllib2

req = urllib2.Request('http://blog.csdn.net/cqcre')
try:
    urllib2.urlopen(req)
except urllib2.HTTPError, e:
    print e.code
except urllib2.URLError, e:
    print e.reason
else:
    print ("OK")
#样例检错：403 
```

## 14.Opener概念

> Cookie，指某些网站为了辨别用户身份、进行session跟踪而储存在用户本地终端上的数据（通常经过加密）比如说有些网站需要登录后才能访问某个页面，在登录之前，你想抓取某个页面内容是不允许的。那么我们可以利用Urllib2库保存我们登录的Cookie，然后再抓取其他页面就达到目的了。

> 当你获取一个URL你使用一个opener(一个urllib2.OpenerDirector的实例)。在前面，我们都是使用的默认的opener，也就是urlopen。它是一个特殊的opener，可以理解成opener的一个特殊实例，传入的参数仅仅是url，data，timeout。

> 如果我们需要用到Cookie，只用这个opener是不能达到目的的，所以我们需要创建更一般的opener来实现对Cookie的设置。

## 15.Cookielib

该模块主要的对象有CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar。

1.获取Cookie保存到变量

```python
import urllib2
import cookielib
#声明一个CookieJar对象实例来保存cookie
cookie = cookielib.CookieJar()
#利用urllib2库的HTTPCookieProcessor对象来创建cookie处理器
handler=urllib2.HTTPCookieProcessor(cookie)
#通过handler来构建opener
opener = urllib2.build_opener(handler)
#此处的open方法同urllib2的urlopen方法，也可以传入request
response = opener.open('http://www.baidu.com')
for item in cookie:
    print 'Name = '+item.name
    print 'Value = '+item.value
```

2.保存Cookie到文件

```python
import cookielib
import urllib2

#设置保存cookie的文件，同级目录下的cookie.txt
filename = 'cookie.txt'
#声明一个MozillaCookieJar对象实例来保存cookie，之后写入文件
cookie = cookielib.MozillaCookieJar(filename)
#利用urllib2库的HTTPCookieProcessor对象来创建cookie处理器
handler = urllib2.HTTPCookieProcessor(cookie)
#通过handler来构建opener
opener = urllib2.build_opener(handler)
#创建一个请求，原理同urllib2的urlopen
response = opener.open("http://www.baidu.com")
#保存cookie到文件
cookie.save(ignore_discard=True, ignore_expires=True)
#ignore_discard的意思是即使cookies将被丢弃也将它保存下来
#ignore_expires的意思是如果该文件中的cookie已经存在，则覆盖原文件写入
```

3.从文件中获取Cookie 值并访问

```python
import cookielib
import urllib2

#创造一个MozillaCookieJar实例对象
cookie = cookielib.MozillaCookieJar()
#从文件中读入值到实例对象
cookie.load('cookie.txt', ignore_disgard = True, ignore_expires = True)
#创造请求的request
request = urllib2.Request("http://www.baidu.com")
#创建一个opener
opener = urllib2.bulid_opener(urllib2.HTTPCookieProcessor(cookie))
reponse = opener.open(request)
print reponse.read()
```

4.利用cookie模拟网站的登录

```python
#示例。。登录校园网
import urllib
import http.cookiejar

filename = 'cookie.txt'

cookie = http.cookiejar.MozillaCookieJar(filename)

opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookie))

postdata = urllib.parse.urlencode({
        'text' : '2016xxxxxxxx',
        'password'   : 'xxxxxxxxxxx'
})

loginurl = 'http://ids.scuec.edu.cn/amserver/UI/Login?goto=http://eol.scuec.edu.cn/meol/homepage/common/sso_login.jsp'

result = opener.open(loginurl, postdata.encode('utf-8'))

cookie.save(ignore_discard=True,ignore_expires=True)

gradeurl = 'http://eol.scuec.edu.cn/meol/jpk/course/layout/newpage/index.jsp?courseId=16574'

result = opener.open(gradeurl)

print ((result.read()).decode('gbk'))
```


## 16.利用正则表达式

#### 1.定义：正则表达式是对字符串操作的一种逻辑公式，就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个“规则字符串”，这个“规则字符串”用来表达对字符串的一种过滤逻辑。 

![正则表达式的语法规则](http://omg5mjb8v.bkt.clouddn.com/20130515113723855-e1424095177180.png)

#### 2.正则表达式的相关注解：

（1）数量词的贪婪模式与非贪婪模式

正则表达式通常用于在文本中查找匹配的字符串。Python里数量词默认是贪婪的（在少数语言里也可能是默认非贪婪），总是尝试匹配尽可能多的字符；非贪婪的则相反，总是尝试匹配尽可能少的字符。例如：正则表达式”ab*”如果用于查找”abbbc”，将找到”abbb”。而如果使用非贪婪的数量词”ab*?”，将找到”a”。

注：我们一般使用非贪婪模式来提取。

### （2）反斜杠问题

与大多数编程语言相同，正则表达式里使用”\”作为转义字符，这就可能造成反斜杠困扰。假如你需要匹配文本中的字符”\”，那么使用编程语言表示的正则表达式里将需要4个反斜杠”\\\\”：前两个和后两个分别用于在编程语言里转义成反斜杠，转换成两个反斜杠后再在正则表达式里转义成一个反斜杠。

Python里的原生字符串很好地解决了这个问题，这个例子中的正则表达式可以使用r”\\”表示。同样，匹配一个数字的”\\d”可以写成r”\d”。有了原生字符串，妈妈也不用担心是不是漏写了反斜杠，写出来的表达式也更直观了。

## 17.两种表达方式

XPath:/html/body/div[2]/ul/li[1]/img

CSS Selector:body > div.main-content > li:nth-child(1) > img

 ## 18.同步和异步加载

它允许无阻塞资源加载，并且使 onload 启动更快，允许页面内容加载，而不需要刷新页面，也可以根据页面内容延迟加载依赖。

```javascript
//异步加载
<strong>(function() {  
     var s = document.createElement('script');  
     s.type = 'text/javascript';  
     s.async = true;  
     s.src = 'http://yourdomain.com/script.js';  
     var x = document.getElementsByTagName('script')[0];  
     x.parentNode.insertBefore(s, x);  
 })();</strong>  
```

```javascript
//同步加载
<script src="http://XXX.com/script.js"></script>
```

同步模式，又称阻塞模式，会阻止浏览器的后续处理，停止了后续的解析，因此停止了后续的文件加载（如图像）、渲染、代码执行。一般的script标签（不带async等属性）加载时会阻塞浏览器，也就是说，浏览器在下载或执行该js代码块时，后面的标签不会被解析，例如在head中添加一个script，但这个script下载时网络不稳定，很长时间没有下载完成对应的js文件，那么浏览器此时一直等待这个js文件下载，此时页面不会被渲染，用户看到的就是白屏。以前的一般建议是把<script>放在页面末尾</body>之前，这样尽可能减少这种阻塞行为，而先让页面展示出来。

## 19.抓取异步加载的数据

```python
def get_more_data(start, end):
    for one in range(start, end):
        get_data(url+str(one))
        tome.sleep(1)
```

## 20.使用MongoDB进行排版和插入

```python
#使用mongodb进行简单的读取和插入
import pymongo

client = pymongo.MongoClient('localhost', 27017)

DB = client['DB']

sheet_line = DB['sheet_line']

path = '/Users/mac/Desktop/1.md'

with open(path, 'r') as f:
    lines = f.readlines()
    for index,line in enumerate(lines):
        data={
                'line'  : line,
                'index' : index,
                'words' : len(line.split())
             }
        print(data)
        sheet_line.insert_one(data)
```

几种表达式：

> $lt:less than
>
> $gt:greater than
>
> $lte:less than equal
>
> $gte:greater than equal
>
> $ne:not equal

e.g->sheet.find{word:{'$lt':5}},表示找到sheet中所有字数比五小的。

```python
#找出字数小于等于三的行数并输出其内容
for item in sheet_line.find{word:{'$lte':3}}:
    print(item['line'])
```

## 21.爬取大规模数据的工作流分析

![](http://omg5mjb8v.bkt.clouddn.com/7B2E42E6-4839-4CCB-81FB-9D4785BDDA12.png)

在爬取大规模数据的时候，要分模块的去爬取

1.构造一个爬取所有网页的爬虫，将爬取到的网页存储到数据库中

2.再构造一个爬虫从数据中提取网址，爬取单个页面的信息

```python
#可以通过find方法来对不同的网页来进行适配,e.g:
if soup.find('td','t'):
	#进行相关的爬取操作
else:
    pass
#在对数据库进行插入操作的时候，也可以通过键值对的模式
 sheet_line.insert_one({'url' : 'http://www.xxx.com'})
```

```python
#将爬取的单个页面信息插入到数据库中
def get_item_info(url):
    web_data = requests.get(url)
    soup = BeautifulSoup(web_data.text, 'lxml')
    title = soup.title.text
    price = soup.select('span.price.c_f50')[0].text
    date = soup.select('.time')[0].text
    #进一步容错的设置
    area = list(soup.select('.c_25d a')[0].stripped_strings) if soup.find_all('span','c_25d') else None
    item_info.insert_one({'title':title, 'date':date, 'area':area})
```

```python
#对于404页面的判断，e.g:
#404页面示例
<script src="http://www.douyu.com/js/404/jQuery-1.3.2.js" type= "text/javascript">
no_longer_exist = '404' in soup.find('script', type = "text/javascript").get('src').split('/')
#返回一个布尔型来判断，加上一个判断语句加入爬取页面信息的函数即可判断404
```

## 22.进程和线程

### 形象的理解方式：

> 单进程单线程：一个餐馆里一张桌子一个人吃饭
>
> 单进程多线程：一个餐馆里一张桌子多个人吃饭
>
> 多进程单线程：一个餐馆里多张桌子，每张桌子一个人吃饭
>
> 多进程多线程：一个餐馆里多张桌子，每个桌子多个人吃法

## 23.多进程爬虫数据抓取

```python
#需要用到的库
from multiprocessing import Pool
from channel_extract import channel_list

def get_all_links_from(channel):
    for num in range(1,101):
        get_link_from(channel,num)

if __name__ == '__main__':
    pool = Pool()
    pool.map(get_all_links_from,channel_list.split())
```

### map函数

map(function,interable,…):对于可迭代函数'iterable'中的每一个元素应用'function'方法，将结果作为list返回

```python
e.g1->def add100(x):
    	return x + 100
    hh = [11,22,33]
    hhh = map(add100,hh)
#此时hhh的值为[111,122,133]
#如果给出了额外的可迭代参数，则对每个可迭代参数中的元素‘并行’的应用‘function’。
e.g2->def abc(a,b,c)
	  	return a*10000 + b*100 + c
	  list1 = [11,22,33]
      list2 = [44,55,66]
      list3 = [77,88,99]
      hh = map(abc,list1,list2,list3)
#此时hh的值为[114477,225588,336699],在每个list中，取出了下标相同的元素，执行了abc()。
#如果'function'给出的是‘None’，自动假定一个‘identity’函数
>>> list1 = [11,22,33]
>>> map(None,list1)
[11, 22, 33]
>>> list1 = [11,22,33]
>>> list2 = [44,55,66]
>>> list3 = [77,88,99]
>>> map(None,list1,list2,list3)
[(11, 44, 77), (22, 55, 88), (33, 66, 99)]
```

## 24.爬取大规模数据实例代码

```python
#page_parsing.py
from bs4 import BeautifulSoup
import requests
import random
import pymongo

client = pymongo.MongoClient('localhost', 27017)

ganji = client['ganji']

url_list = ganji['url_list']

item_info = ganji['item_info']

headers = {
    'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',
    'Connection' : 'keep-alive'
}

proxy_list = [
    'http://121.232.147.178:9000',
    'http://122.243.11.57:9000',
    'http://121.232.145.163:9000'
]
proxy_ip = random.choice(proxy_list)
proxies = {'http' : proxy_ip}


def get_links_from(channel,pages,who_sells='o'):
    list_view = '{}/{}{}/'.format(channel, str(who_sells), str(pages))
    wb_data = requests.get(channel, headers=headers)
    soup = BeautifulSoup(wb_data.text, 'lxml')
    if soup.find('ul', 'pageLink'):
        for link in soup.select('td.t > a.t'):
            item_link = link.get('href').split('?')[0]
            #url_list.insert_one({'url':item_link})
            #print(item_link)
            get_item_info_from(item_link)
            print('\n')
    else:
        #已经到达最后一页
        pass

def get_item_info_from(url, data=None):
    wb_data = requests.get(url, headers=headers)
    if wb_data.status_code == 404:
        pass
    else:
        try:
            soup = BeautifulSoup(wb_data.text, 'lxml')
            data = {
                'title':soup.title.text.strip(),
                'price':soup.select('.f22.fc-orange.f-type')[0].text.strip(),
                'pub_data':soup.select('.pr-5')[0].text.strip().split()[0],
                'area':list(map(lambda x:x.text, soup.select('ul.det-infor > li > a'))),
                'phoneNumber':soup.select('span.phoneNum-style')[0].text.strip(),
                'url':url
            }
            print(data)
        #except IndexError:
            pass
        except AttributeError:
            pass

#get_item_info_from('http://bj.ganji.com/shouji/29096013665341x.htm')
#get_links_from('http://bj.ganji.com/shouji',2)
```

```python
#channel_extracing.py
import requests
from bs4 import  BeautifulSoup

start_url = 'http://bj.ganji.com/wu'
url_host = 'http://bj.ganji.com'

def get_index_url(url):
    wb_data = requests.get(url)
    soup = BeautifulSoup(wb_data.text, 'lxml')
    links = soup.select('.fenlei > dt > a')
    for link in links:
        page_url = url_host + link.get('href')
        print(page_url)

#get_index_url(start_url)

channel_list = '''
http://bj.ganji.com/jiaju/
http://bj.ganji.com/rirongbaihuo/
http://bj.ganji.com/shouji/
http://bj.ganji.com/shoujihaoma/
http://bj.ganji.com/bangong/
http://bj.ganji.com/nongyongpin/
http://bj.ganji.com/jiadian/
http://bj.ganji.com/ershoubijibendiannao/
http://bj.ganji.com/ruanjiantushu/
http://bj.ganji.com/yingyouyunfu/
http://bj.ganji.com/diannao/
http://bj.ganji.com/xianzhilipin/
http://bj.ganji.com/fushixiaobaxuemao/
http://bj.ganji.com/meironghuazhuang/
http://bj.ganji.com/shuma/
http://bj.ganji.com/laonianyongpin/
http://bj.ganji.com/xuniwupin/
http://bj.ganji.com/qitawupin/
http://bj.ganji.com/ershoufree/
http://bj.ganji.com/wupinjiaohuan/
'''
```

```python
#main.py
from multiprocessing import Pool
from channel_extracing import channel_list
from page_parsing import get_item_info_from,get_links_from,url_list,item_info

def get_all_links(channel):
    for i in range(1, 100):
        get_links_from(channel, i)

if __name__ == '__main__':
    pool = Pool()
    pool.map(get_all_links, channel_list.split())
    pool.close()
    pool.join()
```

## 25.更新数据库

```python
db.collection.update()
#update函数的用法,一般传入两个参数
update({id:1},{$set:{name:2}})
```

## 26.突破爬虫封禁的几种方法[参考](http://bigsec.com/bigsec-news/wechat-2016-web-crawler?ref=bigsec-news1)

1.构造合理的HTTP头部请求

2.学会正确的设置cookie

3.正确的时间访问路径（不能访问过快）

4.隐含输入字段值（honey pot）

5.使用可变的远程ip（Tor代理服务器，防止ip被ban）

6.动态页面模拟人为操作（selenium+phantomJS框架）


