---
title: 网络爬虫(二)
date: 2017-05-12 19:58:21
tags: 数据库和HighCharts调用
categories: Python MongoDB charts
author: Peterpan
---
学习笔记
<!--more-->
## 19.抓取异步加载的数据

```python
def get_more_data(start, end):
    for one in range(start, end):
        get_data(url+str(one))
        tome.sleep(1)
```

## 20.使用MongoDB进行排版和插入

```python
#使用mongodb进行简单的读取和插入
import pymongo

client = pymongo.MongoClient('localhost', 27017)

DB = client['DB']

sheet_line = DB['sheet_line']

path = '/Users/mac/Desktop/1.md'

with open(path, 'r') as f:
    lines = f.readlines()
    for index,line in enumerate(lines):
        data={
                'line'  : line,
                'index' : index,
                'words' : len(line.split())
             }
        print(data)
        sheet_line.insert_one(data)
```

几种表达式：

> $lt:less than
>
> $gt:greater than
>
> $lte:less than equal
>
> $gte:greater than equal
>
> $ne:not equal

e.g->sheet.find{word:{'$lt':5}},表示找到sheet中所有字数比五小的。

```python
#找出字数小于等于三的行数并输出其内容
for item in sheet_line.find{word:{'$lte':3}}:
    print(item['line'])
```

## 21.爬取大规模数据的工作流分析

![](http://omg5mjb8v.bkt.clouddn.com/7B2E42E6-4839-4CCB-81FB-9D4785BDDA12.png)

在爬取大规模数据的时候，要分模块的去爬取

1.构造一个爬取所有网页的爬虫，将爬取到的网页存储到数据库中

2.再构造一个爬虫从数据中提取网址，爬取单个页面的信息

```python
#可以通过find方法来对不同的网页来进行适配,e.g:
if soup.find('td','t'):
	#进行相关的爬取操作
else:
    pass
#在对数据库进行插入操作的时候，也可以通过键值对的模式
 sheet_line.insert_one({'url' : 'http://www.xxx.com'})
```

```python
#将爬取的单个页面信息插入到数据库中
def get_item_info(url):
    web_data = requests.get(url)
    soup = BeautifulSoup(web_data.text, 'lxml')
    title = soup.title.text
    price = soup.select('span.price.c_f50')[0].text
    date = soup.select('.time')[0].text
    #进一步容错的设置
    area = list(soup.select('.c_25d a')[0].stripped_strings) if soup.find_all('span','c_25d') else None
    item_info.insert_one({'title':title, 'date':date, 'area':area})
```

```python
#对于404页面的判断，e.g:
#404页面示例
<script src="http://www.douyu.com/js/404/jQuery-1.3.2.js" type= "text/javascript">
no_longer_exist = '404' in soup.find('script', type = "text/javascript").get('src').split('/')
#返回一个布尔型来判断，加上一个判断语句加入爬取页面信息的函数即可判断404
```

## 22.进程和线程

### 形象的理解方式：

> 单进程单线程：一个餐馆里一张桌子一个人吃饭
>
> 单进程多线程：一个餐馆里一张桌子多个人吃饭
>
> 多进程单线程：一个餐馆里多张桌子，每张桌子一个人吃饭
>
> 多进程多线程：一个餐馆里多张桌子，每个桌子多个人吃法

## 23.多进程爬虫数据抓取

```python
#需要用到的库
from multiprocessing import Pool
from channel_extract import channel_list

def get_all_links_from(channel):
    for num in range(1,101):
        get_link_from(channel,num)

if __name__ == '__main__':
    pool = Pool()
    pool.map(get_all_links_from,channel_list.split())
```

### map函数

map(function,interable,…):对于可迭代函数'iterable'中的每一个元素应用'function'方法，将结果作为list返回

```python
e.g1->def add100(x):
    	return x + 100
    hh = [11,22,33]
    hhh = map(add100,hh)
#此时hhh的值为[111,122,133]
#如果给出了额外的可迭代参数，则对每个可迭代参数中的元素‘并行’的应用‘function’。
e.g2->def abc(a,b,c)
	  	return a*10000 + b*100 + c
	  list1 = [11,22,33]
      list2 = [44,55,66]
      list3 = [77,88,99]
      hh = map(abc,list1,list2,list3)
#此时hh的值为[114477,225588,336699],在每个list中，取出了下标相同的元素，执行了abc()。
#如果'function'给出的是‘None’，自动假定一个‘identity’函数
>>> list1 = [11,22,33]
>>> map(None,list1)
[11, 22, 33]
>>> list1 = [11,22,33]
>>> list2 = [44,55,66]
>>> list3 = [77,88,99]
>>> map(None,list1,list2,list3)
[(11, 44, 77), (22, 55, 88), (33, 66, 99)]
```

## 24.爬取大规模数据实例代码

```python
#page_parsing.py
from bs4 import BeautifulSoup
import requests
import random
import pymongo

client = pymongo.MongoClient('localhost', 27017)

ganji = client['ganji']

url_list = ganji['url_list']

item_info = ganji['item_info']

headers = {
    'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',
    'Connection' : 'keep-alive'
}

proxy_list = [
    'http://121.232.147.178:9000',
    'http://122.243.11.57:9000',
    'http://121.232.145.163:9000'
]
proxy_ip = random.choice(proxy_list)
proxies = {'http' : proxy_ip}


def get_links_from(channel,pages,who_sells='o'):
    list_view = '{}/{}{}/'.format(channel, str(who_sells), str(pages))
    wb_data = requests.get(channel, headers=headers)
    soup = BeautifulSoup(wb_data.text, 'lxml')
    if soup.find('ul', 'pageLink'):
        for link in soup.select('td.t > a.t'):
            item_link = link.get('href').split('?')[0]
            #url_list.insert_one({'url':item_link})
            #print(item_link)
            get_item_info_from(item_link)
            print('\n')
    else:
        #已经到达最后一页
        pass

def get_item_info_from(url, data=None):
    wb_data = requests.get(url, headers=headers)
    if wb_data.status_code == 404:
        pass
    else:
        try:
            soup = BeautifulSoup(wb_data.text, 'lxml')
            data = {
                'title':soup.title.text.strip(),
                'price':soup.select('.f22.fc-orange.f-type')[0].text.strip(),
                'pub_data':soup.select('.pr-5')[0].text.strip().split()[0],
                'area':list(map(lambda x:x.text, soup.select('ul.det-infor > li > a'))),
                'phoneNumber':soup.select('span.phoneNum-style')[0].text.strip(),
                'url':url
            }
            print(data)
        #except IndexError:
            pass
        except AttributeError:
            pass

#get_item_info_from('http://bj.ganji.com/shouji/29096013665341x.htm')
#get_links_from('http://bj.ganji.com/shouji',2)
```

```python
#channel_extracing.py
import requests
from bs4 import  BeautifulSoup

start_url = 'http://bj.ganji.com/wu'
url_host = 'http://bj.ganji.com'

def get_index_url(url):
    wb_data = requests.get(url)
    soup = BeautifulSoup(wb_data.text, 'lxml')
    links = soup.select('.fenlei > dt > a')
    for link in links:
        page_url = url_host + link.get('href')
        print(page_url)

#get_index_url(start_url)

channel_list = '''
http://bj.ganji.com/jiaju/
http://bj.ganji.com/rirongbaihuo/
http://bj.ganji.com/shouji/
http://bj.ganji.com/shoujihaoma/
http://bj.ganji.com/bangong/
http://bj.ganji.com/nongyongpin/
http://bj.ganji.com/jiadian/
http://bj.ganji.com/ershoubijibendiannao/
http://bj.ganji.com/ruanjiantushu/
http://bj.ganji.com/yingyouyunfu/
http://bj.ganji.com/diannao/
http://bj.ganji.com/xianzhilipin/
http://bj.ganji.com/fushixiaobaxuemao/
http://bj.ganji.com/meironghuazhuang/
http://bj.ganji.com/shuma/
http://bj.ganji.com/laonianyongpin/
http://bj.ganji.com/xuniwupin/
http://bj.ganji.com/qitawupin/
http://bj.ganji.com/ershoufree/
http://bj.ganji.com/wupinjiaohuan/
'''
```

```python
#main.py
from multiprocessing import Pool
from channel_extracing import channel_list
from page_parsing import get_item_info_from,get_links_from,url_list,item_info

def get_all_links(channel):
    for i in range(1, 100):
        get_links_from(channel, i)

if __name__ == '__main__':
    pool = Pool()
    pool.map(get_all_links, channel_list.split())
    pool.close()
    pool.join()
```

## 25.更新数据库

```python
db.collection.update()
#update函数的用法,一般传入两个参数
update({id:1},{$set:{name:2}})
```

## 26.突破爬虫封禁的几种方法[参考](http://bigsec.com/bigsec-news/wechat-2016-web-crawler?ref=bigsec-news1)

1.构造合理的HTTP头部请求

2.学会正确的设置cookie

3.正确的时间访问路径（不能访问过快）

4.隐含输入字段值（honey pot）

5.使用可变的远程ip（Tor代理服务器，防止ip被ban）

6.动态页面模拟人为操作（selenium+phantomJS框架）

